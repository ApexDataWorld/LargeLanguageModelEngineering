{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f56afc-bc15-46a4-8eb1-d940c332cf52",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "working with meta-llama/Llama-2-7b-chat-hf (and instruct variants) plus other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9289ba7-200c-43a9-b67a-c5ce826c9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7238b278-adfa-4855-b21c-14b95a3e4b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokens from environment variables\n",
    "hf_token = os.environ.get(\"HF_TOKEN\")\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4381e15-e7d7-4775-b5bf-0f6ecaf1404f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19632e4c-e257-474f-97af-a9359d635126",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3f151b-f1cd-4fdf-b522-f07ec14778c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [1, 306, 626, 24173, 304, 1510, 25159, 19427, 297, 3158, 304, 590, 365, 26369, 6012, 414]\n",
      "Decoded: <s> I am excited to show Tokenizers in action to my LLM engineers\n",
      "Token Count: 16\n",
      "Batch Decoded: ['<s>', 'I', 'am', 'excited', 'to', 'show', 'Token', 'izers', 'in', 'action', 'to', 'my', 'L', 'LM', 'engine', 'ers']\n"
     ]
    }
   ],
   "source": [
    "llama_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llama_model, trust_remote_code=True)\n",
    "sample_text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "tokens = tokenizer.encode(sample_text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded:\", tokenizer.decode(tokens))\n",
    "print(\"Token Count:\", len(tokens))\n",
    "print(\"Batch Decoded:\", tokenizer.batch_decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851631e4-34f2-4e88-927e-a6c90635859d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd99b9f9bbb4a06af9ce6d6e04b0c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacb94de0e9b4423ad92db6ffa0f379a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ec309c0aaa4d0b9e529011423f106c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b4c8f999e34ffdae6c0320a6ddaa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat prompt template:\n",
      " <s>[INST] <<SYS>>\n",
      "You are a helpful assistant\n",
      "<</SYS>>\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientists [/INST]\n"
     ]
    }
   ],
   "source": [
    "instruct_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer_instruct = AutoTokenizer.from_pretrained(instruct_model, trust_remote_code=True)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "]\n",
    "prompt = tokenizer_instruct.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(\"Chat prompt template:\\n\", prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d198ce60-b84d-48fa-a3e2-664da59faf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) Using alternative models: Phi3, Qwen2, Starcoder2\n",
    "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
    "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "503c7fff-9443-4b87-acd9-85e0df4391a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c921935d7f064e0d88634baff94def7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f47976b75149b1afc025cf4bb7c4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a686bca7e4bb43b1822d8d365b70dcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0ec142ba2453f889d43d6ba5207e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6447053cab494e8da8320972ca15ba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f98c4568fc41139dbb9244a9e63cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a394a538f4ac4a8cb29d7cd423a4b1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e702e785644284b297d0afb9b2f5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc18bcc169c94fe199ffcd335948adb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6569e345aeae43078d050ee20a626be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f825efffa440a99fbe065cbc513e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66744b5989614f3e94c2138e92404afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4918b757ca4aaf9b09ddc1436e4195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb46f3505a641d2ae875ef74133efa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
    "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
    "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7980322e-e505-4736-b48f-657688c8ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama tokens: [1, 306, 626, 24173, 304, 1510, 25159, 19427, 297, 3158, 304, 590, 365, 26369, 6012, 414]\n",
      "Phi3 tokens: [306, 626, 24173, 304, 1510, 25159, 19427, 297, 3158, 304, 590, 365, 26369, 6012, 414]\n",
      "Qwen2 tokens: [40, 1079, 12035, 311, 1473, 9660, 12230, 304, 1917, 311, 847, 444, 10994, 24198]\n",
      "Chat prompt (Phi3): <|system|>\n",
      "You are a helpful assistant<|end|>\n",
      "<|user|>\n",
      "Tell a light-hearted joke for a room of Data Scientists<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "Chat prompt (Qwen2): <|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Meta-Llama tokens:\", tokenizer.encode(sample_text))\n",
    "print(\"Phi3 tokens:\", phi3_tokenizer.encode(sample_text))\n",
    "print(\"Qwen2 tokens:\", qwen2_tokenizer.encode(sample_text))\n",
    "print(\"Chat prompt (Phi3):\", phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "print(\"Chat prompt (Qwen2):\", qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2920194e-0f42-48ab-b6c2-61d5995613f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starcoder2 tokens for code:\n",
      "222 = \n",
      "\n",
      "610 = def\n",
      "17966 =  hello\n",
      "100 = _\n",
      "5879 = world\n",
      "45 = (\n",
      "6427 = person\n",
      "731 = ):\n",
      "303 = \n",
      "   \n",
      "1489 =  print\n",
      "459 = (\"\n",
      "8302 = Hello\n",
      "411 = \",\n",
      "4944 =  person\n",
      "46 = )\n",
      "222 = \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_code = \"\"\"\n",
    "def hello_world(person):\n",
    "    print(\"Hello\", person)\n",
    "\"\"\"\n",
    "tokens_code = starcoder2_tokenizer.encode(sample_code)\n",
    "print(\"Starcoder2 tokens for code:\")\n",
    "for token in tokens_code:\n",
    "    print(f\"{token} = {starcoder2_tokenizer.decode(token)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
